just an attempt to try to create a local RAG system that can help easily understand complex documents

## Specifications:

- embedding model: `mxbai-embed-large`
- inference model: `qwen2.5:7b-instruct-q8_0` (tried mixtral large and phi3 and llama3, at q4 and q8(wherever available). Qwen models felt like the best. )
- model server was run using `ollama`  (it is really hard to run these models via code and keep them dynamic during this POC stage, might pickup later)
- mostly built with `langchain` (no preference, just the first framework that got suggested)

---

_requirements.txt generated using `pip3 freeze > requirements.txt`_